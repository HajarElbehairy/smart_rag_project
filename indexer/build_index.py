import os
import json
import faiss
import numpy as np
import google.generativeai as genai
import hashlib
from datetime import datetime

# ================= CONFIG =================
INPUT_DIR = "chunks"                 # ÙÙˆÙ„Ø¯Ø± ÙÙŠÙ‡ JSON chunks
FAISS_FILE = "aiss_index.index"     # Ù…Ù„Ù Ø§Ù„Ù€ FAISS index
META_FILE = "meta.json"              # Ù…Ù„Ù metadata
INDEX_INFO_FILE = "index_info.json"  # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«
BATCH_SIZE = 16                      

# Gemini API setup
genai.configure(api_key=os.environ.get("GEMINI_API_KEY", "AIzaSyAsMvD5vJbNl21Qc5NdIXfl2bw_D4ZOYAw"))

# ================= HELPER FUNCTIONS =================
def get_gemini_embeddings(texts, model="models/text-embedding-004"):
    """Compute embeddings for a batch of texts using Gemini"""
    embeddings = []
    for text in texts:
        try:
            resp = genai.embed_content(
                model=model, 
                content=text, 
                task_type="retrieval_document"
            )
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙØªØ§Ø­ Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù€ embedding
            embeddings.append(np.array(resp['embedding'], dtype=np.float32))
        except Exception as e:
            print(f"âŒ Error embedding text: {e}")
            # ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£ØŒ Ù†Ø¶ÙŠÙ vector ÙØ§Ø¶ÙŠ Ø¨Ù†ÙØ³ Ø§Ù„Ø­Ø¬Ù…
            if embeddings:
                embeddings.append(np.zeros_like(embeddings[0]))
            else:
                embeddings.append(np.zeros(768, dtype=np.float32))  # default dimension
    return embeddings

def load_chunks(input_dir):
    """Load all JSON chunks from directory"""
    texts, metas = [], []
    
    if not os.path.exists(input_dir):
        print(f"âŒ Directory {input_dir} does not exist!")
        return texts, metas
    
    json_files = [f for f in os.listdir(input_dir) if f.endswith(".json")]
    
    for f in sorted(json_files):  # sorted Ù„Ù„ØªØ±ØªÙŠØ¨
        path = os.path.join(input_dir, f)
        try:
            with open(path, encoding="utf-8") as file:
                data = json.load(file)
            
            texts.append(data.get("text", ""))
            metas.append({
                "url": data.get("url", ""),
                "title": data.get("title", ""),
                "position": data.get("position", 0),
                "checksum": data.get("checksum", ""),
                "filename": f,
                "indexed_at": datetime.now().isoformat()
            })
        except Exception as e:
            print(f"âš ï¸ Error loading {f}: {e}")
    
    return texts, metas

def calculate_directory_hash(input_dir):
    """Calculate hash of all files in directory to detect changes"""
    hash_md5 = hashlib.md5()
    
    if not os.path.exists(input_dir):
        return None
    
    for filename in sorted(os.listdir(input_dir)):
        if filename.endswith(".json"):
            filepath = os.path.join(input_dir, filename)
            with open(filepath, 'rb') as f:
                hash_md5.update(f.read())
    
    return hash_md5.hexdigest()

def should_reindex(input_dir):
    """Check if re-indexing is needed based on content changes"""
    # Ø¥Ø°Ø§ Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯Ø© Ø£ØµÙ„Ø§Ù‹
    if not os.path.exists(FAISS_FILE) or not os.path.exists(META_FILE):
        return True
    
    # Ø¥Ø°Ø§ Ù…Ù„Ù index_info Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯
    if not os.path.exists(INDEX_INFO_FILE):
        return True
    
    # Ù‚Ø±Ø§Ø¡Ø© Ø¢Ø®Ø± hash
    try:
        with open(INDEX_INFO_FILE, 'r', encoding='utf-8') as f:
            index_info = json.load(f)
        last_hash = index_info.get('content_hash')
    except:
        return True
    
    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù€ hash Ø§Ù„Ø­Ø§Ù„ÙŠ
    current_hash = calculate_directory_hash(input_dir)
    
    # Ù…Ù‚Ø§Ø±Ù†Ø©
    if last_hash != current_hash:
        print("ğŸ”„ Content changed detected. Re-indexing needed.")
        return True
    
    print("âœ… Content unchanged. Using existing index.")
    return False

def save_index_info(content_hash, num_chunks):
    """Save indexing information"""
    info = {
        'content_hash': content_hash,
        'num_chunks': num_chunks,
        'indexed_at': datetime.now().isoformat(),
        'model': 'text-embedding-004'
    }
    with open(INDEX_INFO_FILE, 'w', encoding='utf-8') as f:
        json.dump(info, f, ensure_ascii=False, indent=2)

# ================= RE-INDEX FUNCTION =================
def build_faiss_index(force_reindex=False):
    """Build or load FAISS index with smart re-indexing"""
    
    # Check if re-indexing is needed
    if not force_reindex and not should_reindex(INPUT_DIR):
        print("âœ… Loading existing FAISS index...")
        index = faiss.read_index(FAISS_FILE)
        with open(META_FILE, encoding="utf-8") as f:
            metas = json.load(f)
        print(f"ğŸ“Š Loaded index with {index.ntotal} vectors")
        return index, metas

    # Load chunks
    print("ğŸ”¹ Starting indexing process...")
    texts, metas = load_chunks(INPUT_DIR)
    
    if len(texts) == 0:
        print("âŒ No chunks found!")
        return None, []
    
    print(f"ğŸ”¹ Loaded {len(texts)} chunks")

    # Compute embeddings in batches
    all_embeddings = []
    print("ğŸ”¹ Computing embeddings...")
    
    for i in range(0, len(texts), BATCH_SIZE):
        batch_texts = texts[i:i+BATCH_SIZE]
        print(f"   Processing batch {i//BATCH_SIZE + 1}/{(len(texts)-1)//BATCH_SIZE + 1}...")
        batch_embs = get_gemini_embeddings(batch_texts)
        all_embeddings.extend(batch_embs)

    # Convert to numpy array
    embeddings_array = np.stack(all_embeddings)
    dim = embeddings_array.shape[1]
    print(f"ğŸ”¹ Embedding dimension: {dim}")

    # Build FAISS index (L2 distance)
    print("ğŸ”¹ Building FAISS index...")
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings_array)

    # Save FAISS index
    faiss.write_index(index, FAISS_FILE)
    print(f"âœ… FAISS index saved: {FAISS_FILE}")
    
    # Save metadata
    with open(META_FILE, "w", encoding="utf-8") as f:
        json.dump(metas, f, ensure_ascii=False, indent=2)
    print(f"âœ… Metadata saved: {META_FILE}")
    
    # Save index info
    content_hash = calculate_directory_hash(INPUT_DIR)
    save_index_info(content_hash, len(texts))
    print(f"âœ… Index info saved: {INDEX_INFO_FILE}")
    
    print(f"ğŸ“Š Total vectors indexed: {index.ntotal}")
    return index, metas

# ================= SEARCH FUNCTION (Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±) =================
def search_index(query, top_k=5):
    """Search the FAISS index for relevant chunks"""
    if not os.path.exists(FAISS_FILE):
        print("âŒ Index not found! Please build it first.")
        return []
    
    # Load index and metadata
    index = faiss.read_index(FAISS_FILE)
    with open(META_FILE, encoding="utf-8") as f:
        metas = json.load(f)
    
    # Embed query
    print(f"ğŸ” Searching for: {query}")
    query_embedding = get_gemini_embeddings([query])[0]
    query_embedding = np.expand_dims(query_embedding, axis=0)
    
    # Search
    distances, indices = index.search(query_embedding, top_k)
    
    # Return results
    results = []
    for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):
        if idx != -1:  # valid result
            results.append({
                'rank': i + 1,
                'distance': float(dist),
                'metadata': metas[idx]
            })
    
    return results

# ================= MAIN =================
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ FAISS Indexing System")
    print("=" * 60)
    
    # Build or load index
    index, metas = build_faiss_index(force_reindex=False)  # ØºÙŠØ±ÙŠ Ù„Ù€ True Ù„Ùˆ Ø¹Ø§ÙŠØ²Ø© Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡
    
    if index is not None:
        print("\n" + "=" * 60)
        print("âœ… Done. Index ready for retrieval.")
        print("=" * 60)
        
        # Test search (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
        print("\nğŸ§ª Testing search functionality...")
        test_query = "machine learning"
        results = search_index(test_query, top_k=3)
        
        print(f"\nTop results for '{test_query}':")
        for result in results:
            print(f"\n{result['rank']}. Distance: {result['distance']:.4f}")
            print(f"   Title: {result['metadata']['title']}")
            print(f"   URL: {result['metadata']['url']}")
            print(f"   File: {result['metadata']['filename']}")
    else:
        print("âŒ Failed to build index.")